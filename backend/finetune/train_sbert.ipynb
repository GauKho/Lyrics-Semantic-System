{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26794c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"backend\\data\\csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d7a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3608 unique lyrics\n"
     ]
    }
   ],
   "source": [
    "all_lyrics = []\n",
    "csv_file = glob.glob(os.path.join(file_path, \"*.csv\"))\n",
    "min_word = 5\n",
    "\n",
    "for file in csv_file:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df[[\"Artist\", \"Title\", \"Album\", \"Lyric\"]].dropna()\n",
    "\n",
    "        df[\"Album\"] = df[\"Album\"].astype(str).str.strip().str.lower()\n",
    "        df[\"Lyric\"] = df[\"Lyric\"].astype(str).str.strip()\n",
    "\n",
    "        df_cleaned = df[~df[\"Album\"].str.contains(\"unreleased\", case=False, na=False)].copy()\n",
    "        df_cleaned = df_cleaned[df_cleaned[\"Lyric\"].str.split().str.len() >= min_word]\n",
    "\n",
    "\n",
    "        all_lyrics.append(df_cleaned)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "lyrics_df = pd.concat(all_lyrics, ignore_index=True)\n",
    "lyrics_df.drop_duplicates(subset=[\"Lyric\"], inplace=True)\n",
    "print(f\"Loaded {len(lyrics_df)} unique lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6682fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5640\n"
     ]
    }
   ],
   "source": [
    "print(len(lyrics_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a616442",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for _, row in lyrics_df.iterrows():\n",
    "    artists = row[\"Artist\"].strip().lower()\n",
    "    lyrics = row[\"Lyric\"].strip().lower()\n",
    "    titles = row[\"Title\"].strip().lower()\n",
    "    if len(lyrics.split()) > 5:\n",
    "        train_examples.append(InputExample(texts=[lyrics, titles]))\n",
    "        train_examples.append(InputExample(texts=[lyrics, artists]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d6caeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11196\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e1dd7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11196 training pairs to training_data.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_examples, f)\n",
    "\n",
    "print(f\"Saved {len(train_examples)} training pairs to training_data.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ef8e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 700/700 [01:09<00:00, 10.03it/s]\n",
      "Iteration: 100%|██████████| 700/700 [01:09<00:00, 10.08it/s]\n",
      "Epoch: 100%|██████████| 2/2 [02:19<00:00, 69.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model fine-tuned and saved to 'lyrics_sbert_model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load training examples\n",
    "with open(\"training_data.pkl\", \"rb\") as f:\n",
    "    train_examples = pickle.load(f)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# Loss function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Fine-tune\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=2,\n",
    "    warmup_steps=100,\n",
    "    output_path=\"lyrics_sbert_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Model fine-tuned and saved to 'lyrics_sbert_model'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
